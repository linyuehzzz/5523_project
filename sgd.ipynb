{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sgd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHOCkuiQxg1fvfHGjfM//O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linyuehzzz/5523_project/blob/main/sgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlILGcNEEW0E"
      },
      "source": [
        "##**Stochastic Gradient Descent for Logistic Regression**\n",
        "This code implements and tests the SGD algorithm for logistic regression\n",
        "in different scenarios.  \n",
        "Yue Lin (lin.3326 at osu.edu)  \n",
        "Created: 11/12/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3jzXZtrB7Fj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGQmKFIeFYKY"
      },
      "source": [
        "#### **Set up libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUex5ElWFaHO"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu0jhwwRF5ls"
      },
      "source": [
        "#### **Prepare data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTRysHMwfe1D"
      },
      "source": [
        "##### Generate training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSXWx3zyF9B3"
      },
      "source": [
        "def train_data(n_epoch, train_bs):\n",
        "  train_x = np.random.uniform(-1, 1, (train_bs * n_epoch, 4))\n",
        "  b = np.ones((train_bs * n_epoch, 1))\n",
        "  train_x = np.append(train_x, b, axis=1)\n",
        "  train_y = np.array([random.randrange(-1, 2, 2) for i in range(train_bs * n_epoch)])\n",
        "  # print(train_x, train_y)\n",
        "  return train_x, train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHC29iHfY5G"
      },
      "source": [
        "##### Generate test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZNMPtZQe8-t"
      },
      "source": [
        "def test_data(test_n):\n",
        "  test_x = np.random.uniform(-1, 1, (test_n, 4))\n",
        "  b = np.ones((test_n, 1))\n",
        "  test_x = np.append(test_x, b, axis=1)\n",
        "  test_y = np.array([random.randrange(-1, 2, 2) for i in range(test_n)])\n",
        "  # print(test_x, test_y)\n",
        "  return test_x, test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAzwTZ79M6oi"
      },
      "source": [
        "#### **Train**\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "\n",
        "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8XQyQ8Ycqpo"
      },
      "source": [
        "##### Predict using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzPxwbmIM6NU"
      },
      "source": [
        "# Make a prediction with coefficients\n",
        "def pred(X, w):\n",
        "  yhat = w[-1]\n",
        "  for i in range(X.shape[0] - 1):\n",
        "    yhat += w[i] * X[i]\n",
        "  yhat = 1.0 / (1.0 + np.exp(-yhat))\n",
        "  if yhat < 0.5:\n",
        "    yhat = -1\n",
        "  else:\n",
        "    yhat = 1 \n",
        "  return yhat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJfOdxzt4M5T"
      },
      "source": [
        "##### Estimate logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_3nPdw4BJV0"
      },
      "source": [
        "def log_loss(X, y, w):\n",
        "  return np.ln(1 + np.exp(-y * np.dot(w.T, X)))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnfgixZxQqvj"
      },
      "source": [
        "##### Estimate classification error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdcop9KQuGF"
      },
      "source": [
        "def err(yhat, y):\n",
        "  if yhat == y:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft2T2WdzzKNw"
      },
      "source": [
        "##### Project weights (senario 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU5a9ziKzQpy"
      },
      "source": [
        "def prj_1(grad):\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE93wIbQc9Is"
      },
      "source": [
        "##### Estimate weight vector using SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb_QkJsIc8Vh"
      },
      "source": [
        "def train_sgd(train_x, train_y, test_x, test_y, l_rate, n_epoch, bs):\n",
        "  # w = np.zeros(len(train_x[0]))\n",
        "  w = np.zeros(train_x.shape[1])\n",
        "  risk_all = np.zeros(n_epoch)\n",
        "  cls_err_all = np.zeros(n_epoch)\n",
        "\n",
        "  for epoch in range(n_epoch):\n",
        "    risk = cls_err = grad = 0.\n",
        "    for idx in range(epoch * bs, (epoch + 1) * bs):\n",
        "      # Read data\n",
        "      X = train_x[idx]\n",
        "      y = train_y[idx]\n",
        "\n",
        "      # Forward propagation\n",
        "      yhat = pred(X, w)\n",
        "\n",
        "      # Calculate gradient\n",
        "      grad += (-y * X * np.exp(-y * np.dot(w.T, X)) / (1 + np.exp(-y * np.dot(w.T, X)))) / bs\n",
        "    \n",
        "    # Backward propagation\n",
        "    w[-1] = w[-1] + l_rate * grad\n",
        "    for i in range(len(x) - 1):\n",
        "      w[i] = w[i] + l_rate * grad * X[i]\n",
        "    \n",
        "    # Evaluate\n",
        "    for \n",
        "    risk += log_loss(X, y, w)\n",
        "    cls_err += err(yhat, y)\n",
        "\n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_err))\n",
        "\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3SH2Dm7d71V"
      },
      "source": [
        "#### **Wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ68DlKXd6rP",
        "outputId": "4f9bf695-5ce9-446a-ecd2-f57d676c563a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_epoch = 30\n",
        "train_bs = 50\n",
        "test_n = 400\n",
        "l_rate = 0.001\n",
        "\n",
        "# Generate training data\n",
        "train_x, train_y = train_data(n_epoch, train_bs)\n",
        "\n",
        "# Generate test data\n",
        "test_x, test_y = test_data(test_n)\n",
        "\n",
        "# Train\n",
        "w = train_sgd(train_x, train_y, l_rate, n_epoch, train_bs)\n",
        "print(w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch=0, lrate=0.001, error=-26.948\n",
            ">epoch=1, lrate=0.001, error=-41.835\n",
            ">epoch=2, lrate=0.001, error=-68.659\n",
            ">epoch=3, lrate=0.001, error=-91.413\n",
            ">epoch=4, lrate=0.001, error=-126.067\n",
            ">epoch=5, lrate=0.001, error=-158.617\n",
            ">epoch=6, lrate=0.001, error=-175.112\n",
            ">epoch=7, lrate=0.001, error=-191.538\n",
            ">epoch=8, lrate=0.001, error=-221.898\n",
            ">epoch=9, lrate=0.001, error=-256.173\n",
            ">epoch=10, lrate=0.001, error=-262.346\n",
            ">epoch=11, lrate=0.001, error=-286.491\n",
            ">epoch=12, lrate=0.001, error=-312.569\n",
            ">epoch=13, lrate=0.001, error=-342.547\n",
            ">epoch=14, lrate=0.001, error=-364.446\n",
            ">epoch=15, lrate=0.001, error=-380.274\n",
            ">epoch=16, lrate=0.001, error=-412.033\n",
            ">epoch=17, lrate=0.001, error=-433.721\n",
            ">epoch=18, lrate=0.001, error=-459.339\n",
            ">epoch=19, lrate=0.001, error=-488.850\n",
            ">epoch=20, lrate=0.001, error=-516.294\n",
            ">epoch=21, lrate=0.001, error=-547.631\n",
            ">epoch=22, lrate=0.001, error=-576.875\n",
            ">epoch=23, lrate=0.001, error=-600.048\n",
            ">epoch=24, lrate=0.001, error=-619.165\n",
            ">epoch=25, lrate=0.001, error=-646.195\n",
            ">epoch=26, lrate=0.001, error=-661.155\n",
            ">epoch=27, lrate=0.001, error=-690.068\n",
            ">epoch=28, lrate=0.001, error=-706.908\n",
            ">epoch=29, lrate=0.001, error=-719.688\n",
            "[-0.00751394 -0.00319849  0.00242012 -0.00161087 -0.17944302]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sgd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFqxiXR9WLLss14sYplPUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linyuehzzz/5523_project/blob/main/sgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlILGcNEEW0E"
      },
      "source": [
        "##**Stochastic Gradient Descent for Logistic Regression**\n",
        "This code implements and tests the SGD algorithm for logistic regression\n",
        "in different scenarios.  \n",
        "Yifei Zhang, Yue Lin (lin.3326 at osu.edu)  \n",
        "Created: 11/12/2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGQmKFIeFYKY"
      },
      "source": [
        "#### **Set up libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUex5ElWFaHO"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXnNnylopljK"
      },
      "source": [
        "#### **Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzHKV3T7pvFo"
      },
      "source": [
        "##### Projection function for hypercube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ewruRRFp0kP"
      },
      "source": [
        "def cube_prj(sample):\n",
        "  '''\n",
        "  input: a sample with d dimension (1-D array or list)\n",
        "  onput: the euclidean projection of sample\n",
        "  '''\n",
        "  return [np.sign(i) * min(np.abs(i), 1) for i in sample]"
      ],
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mrnAQvYqI8q"
      },
      "source": [
        "##### Projection function for unit ball"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyUbRJAKqRsd"
      },
      "source": [
        "def ball_prj(sample):\n",
        "    '''\n",
        "    input: a sample with d dimension (1-D array or list)\n",
        "    onput: the euclidean projection of sample\n",
        "    '''\n",
        "    ratio = 1 / np.linalg.norm(sample)\n",
        "    return [i * ratio for i in sample]"
      ],
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZXfvRgKqXV2"
      },
      "source": [
        "##### Project data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dwzkd6WqW7X"
      },
      "source": [
        "def prj_data(x, y, prj_code):\n",
        "    '''\n",
        "    This function is for conduct projection on data array\n",
        "    x: n*d array (n is sample#, d is dimension)\n",
        "    y: 1-d array with label of -1 or +1\n",
        "    prj_code: type of projection, 0 for cube, 1 for ball\n",
        "    return:\n",
        "        prj_x: projected x \n",
        "        y: same as input\n",
        "    '''\n",
        "    if prj_code == 0:\n",
        "        prj_x = np.apply_along_axis(cube_prj, 1, x)\n",
        "    elif prj_code == 1:\n",
        "        prj_x = np.apply_along_axis(ball_prj, 1, x)\n",
        "    else:\n",
        "        print(\"Please input correct code for projection type: 0 for cube, 1 for ball\")\n",
        "      \n",
        "    b = np.ones((prj_x.shape[0], 1))\n",
        "    prj_x = np.append(prj_x, b, axis=1)\n",
        "    return prj_x, y"
      ],
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiUQqYKLvAuY"
      },
      "source": [
        "##### Project gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ThWld5kvaBT"
      },
      "source": [
        "def prj_grad(g, prj_code):\n",
        "    '''\n",
        "    This function is for conduct projection on gradients\n",
        "    g: 1-d array (d is dimension)\n",
        "    prj_code: type of projection, 0 for cube, 1 for ball\n",
        "    return:\n",
        "        prj_g: projected gradient\n",
        "    '''\n",
        "    if prj_code == 0:\n",
        "        prj_g = cube_prj(g)\n",
        "    elif prj_code == 1:\n",
        "        prj_g = ball_prj(g)\n",
        "    else:\n",
        "        print(\"Please input correct code for projection type: 0 for cube, 1 for ball\")\n",
        "    return prj_g"
      ],
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu0jhwwRF5ls"
      },
      "source": [
        "#### **Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMDzFiFcrsg8"
      },
      "source": [
        "def gen_data(sig, n):\n",
        "    '''\n",
        "    The function is to generate data for training and testing\n",
        "    The feature array is 4 dimension array. \n",
        "     + Each feature follows the Normal distribution(mu,sig)\n",
        "     + with probability 1/2, the y =1 , \n",
        "         generate the correspoinding feature vector from N(mu,sig),mu is [1/4,1/4,1/4,1/4],sig is set as you need.\n",
        "    sig: the sigma of  gussian vector\n",
        "    n: the sample number\n",
        "    \n",
        "    Return:\n",
        "     x: n*d_dimension array\n",
        "     y: 1-d dimension array with -1 and +1\n",
        "    '''\n",
        "    d_dimension = 4\n",
        "    y = np.random.choice([-1, 1], p = [0.5, 0.5], size = n)\n",
        "    x = np.array([])\n",
        "    for i in range(n):\n",
        "        if y[i] == -1:\n",
        "            mu = -(1 / 4)\n",
        "            negvec = np.random.normal(mu, sig, d_dimension)\n",
        "            x = np.concatenate([x, negvec], axis=0)\n",
        "        else:\n",
        "            mu = (1/4)\n",
        "            posvec = np.random.normal(mu, sig, d_dimension)\n",
        "            x = np.concatenate([x,posvec], axis=0)\n",
        "    x = np.reshape(x, (n, d_dimension))\n",
        "    return x, y"
      ],
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAzwTZ79M6oi"
      },
      "source": [
        "#### **Train**\n",
        "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8XQyQ8Ycqpo"
      },
      "source": [
        "##### Predict using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzPxwbmIM6NU"
      },
      "source": [
        "# Make a prediction with coefficients\n",
        "def pred(X, w):\n",
        "  yhat = 0.\n",
        "  for i in range(X.shape[0]):\n",
        "    yhat += w[i] * X[i]\n",
        "  yhat = 1.0 / (1.0 + np.exp(-yhat))\n",
        "  if yhat < 0.5:\n",
        "    yhat = -1\n",
        "  else:\n",
        "    yhat = 1 \n",
        "  return yhat"
      ],
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJfOdxzt4M5T"
      },
      "source": [
        "##### Estimate logistic loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_3nPdw4BJV0"
      },
      "source": [
        "def log_loss(X, y, w):\n",
        "  return np.log(1 + np.exp(-y * np.dot(w.T, X)))"
      ],
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnfgixZxQqvj"
      },
      "source": [
        "##### Estimate classification error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdcop9KQuGF"
      },
      "source": [
        "def err(yhat, y):\n",
        "  if yhat == y:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE93wIbQc9Is"
      },
      "source": [
        "##### Estimate weight vector using SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb_QkJsIc8Vh"
      },
      "source": [
        "def train_sgd(train_x, train_y, test_x, test_y, l_rate, n_epoch, bs, prj_code):\n",
        "  w = np.random.uniform(-1, 1, (train_x.shape[1]))\n",
        "  print(w)\n",
        "  risk_all = np.zeros(n_epoch)\n",
        "  cls_err_all = np.zeros(n_epoch)\n",
        "\n",
        "  for epoch in range(n_epoch):\n",
        "    risk = cls_err = grad = 0.\n",
        "    for idx in range(epoch * bs, (epoch + 1) * bs):\n",
        "      # Read data\n",
        "      X = train_x[idx]\n",
        "      y = train_y[idx]\n",
        "      # Calculate gradient\n",
        "      g = (-y * X * np.exp(-y * np.dot(w.T, X)) / (1 + np.exp(-y * np.dot(w.T, X))))\n",
        "      grad += g\n",
        "\n",
        "    # Project gradient\n",
        "    grad = prj_grad(grad / bs, prj_code)\n",
        "    # Backward propagation\n",
        "    w = np.add(w, np.multiply(-l_rate, grad))\n",
        "    \n",
        "    # Evaluate\n",
        "    for idx in range(test_x.shape[0]):\n",
        "      # Read data\n",
        "      X = test_x[idx]\n",
        "      y = test_y[idx]\n",
        "      # Predict\n",
        "      yhat = pred(X, w)\n",
        "      # Evaluate\n",
        "      risk += log_loss(X, y, w) / test_x.shape[0]\n",
        "      cls_err += err(yhat, y) / test_x.shape[0]\n",
        "    \n",
        "    risk_all = np.append(risk_all, risk, axis=None)\n",
        "    cls_err_all = np.append(cls_err_all, cls_err, axis=None)\n",
        "    print('>epoch=%d, lrate=%.3f, risk=%.3f, classification error=%.3f' % (epoch, l_rate, risk, cls_err))\n",
        "  \n",
        "  # Report risk\n",
        "  risk_ave = np.average(risk_all)\n",
        "  risk_min = np.amin(risk_all)\n",
        "  risk_var = np.var(risk_all)\n",
        "  exp_excess_risk = risk_ave - risk_min\n",
        "  # Report classification error\n",
        "  cls_err_ave = np.average(cls_err_all)\n",
        "  cls_err_var = np.var(cls_err_all)\n",
        "  return [w, risk_ave, risk_min, risk_var, exp_excess_risk, cls_err_ave, cls_err_var]"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3SH2Dm7d71V"
      },
      "source": [
        "#### **Wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ68DlKXd6rP",
        "outputId": "7b1f8eb6-ad54-4d85-c6d9-8ca9633773aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fixed hyperparameters\n",
        "n_epoch = 30    # training epochs\n",
        "test_n = 400    # size of test set\n",
        "\n",
        "# Unfixed hyperparameters\n",
        "prj_code = 1    # code for two scenario: 0 for cube, 1 for ball\n",
        "l_rate = 0.04  # learning rate: 0.04 for cube, 0.18 for ball\n",
        "train_bs = np.array([50, 100, 500, 1000])   # batch size for each training epoch\n",
        "sigma = 0.1     # variance of Gaussian distribution\n",
        "\n",
        "excess_risk = np.zeros(train_bs.shape[0])\n",
        "cls_error = np.zeros(train_bs.shape[0])\n",
        "\n",
        "for bs in train_bs:\n",
        "  # Generate training data\n",
        "  train_x, train_y = gen_data(sigma, bs * n_epoch)\n",
        "  train_px, train_py = prj_data(train_x, train_y, prj_code)\n",
        "\n",
        "  # Generate test data\n",
        "  test_x, test_y = gen_data(sigma, test_n)\n",
        "  test_px, test_py = prj_data(test_x, test_y, prj_code)\n",
        "\n",
        "  # Train\n",
        "  output = train_sgd(train_px, train_py, test_px, test_py, l_rate, n_epoch, bs, prj_code)\n",
        "  print('>scenario=%d, sigma=%.2f, n=%d, log_loss_mean=%.3f, log_loss_std_dev=%.3f, log_loss_min=%.3f, \\\n",
        "        excess_risk=%.3f, cls_error_mean=%.3f, cls_error_std_dev=%.3f' \n",
        "        % (prj_code+1, sigma, bs, output[1], output[3], output[2], output[4], output[5], output[6]))\n",
        "  \n",
        "  excess_risk = np.append(excess_risk, output[4], axis=None)\n",
        "  cls_error = np.append(cls_error, output[5],  axis=None)\n",
        "  "
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.29899861 -0.50436514  0.63478566 -0.71664263 -0.30975044]\n",
            ">epoch=0, lrate=0.040, risk=0.781, classification error=0.610\n",
            ">epoch=1, lrate=0.040, risk=0.760, classification error=0.595\n",
            ">epoch=2, lrate=0.040, risk=0.740, classification error=0.567\n",
            ">epoch=3, lrate=0.040, risk=0.721, classification error=0.540\n",
            ">epoch=4, lrate=0.040, risk=0.702, classification error=0.515\n",
            ">epoch=5, lrate=0.040, risk=0.683, classification error=0.463\n",
            ">epoch=6, lrate=0.040, risk=0.664, classification error=0.423\n",
            ">epoch=7, lrate=0.040, risk=0.646, classification error=0.385\n",
            ">epoch=8, lrate=0.040, risk=0.629, classification error=0.340\n",
            ">epoch=9, lrate=0.040, risk=0.611, classification error=0.273\n",
            ">epoch=10, lrate=0.040, risk=0.595, classification error=0.235\n",
            ">epoch=11, lrate=0.040, risk=0.578, classification error=0.215\n",
            ">epoch=12, lrate=0.040, risk=0.562, classification error=0.193\n",
            ">epoch=13, lrate=0.040, risk=0.546, classification error=0.165\n",
            ">epoch=14, lrate=0.040, risk=0.531, classification error=0.125\n",
            ">epoch=15, lrate=0.040, risk=0.515, classification error=0.100\n",
            ">epoch=16, lrate=0.040, risk=0.500, classification error=0.075\n",
            ">epoch=17, lrate=0.040, risk=0.486, classification error=0.053\n",
            ">epoch=18, lrate=0.040, risk=0.471, classification error=0.040\n",
            ">epoch=19, lrate=0.040, risk=0.458, classification error=0.032\n",
            ">epoch=20, lrate=0.040, risk=0.445, classification error=0.025\n",
            ">epoch=21, lrate=0.040, risk=0.432, classification error=0.015\n",
            ">epoch=22, lrate=0.040, risk=0.419, classification error=0.007\n",
            ">epoch=23, lrate=0.040, risk=0.406, classification error=0.005\n",
            ">epoch=24, lrate=0.040, risk=0.393, classification error=0.005\n",
            ">epoch=25, lrate=0.040, risk=0.381, classification error=0.003\n",
            ">epoch=26, lrate=0.040, risk=0.369, classification error=0.003\n",
            ">epoch=27, lrate=0.040, risk=0.358, classification error=0.003\n",
            ">epoch=28, lrate=0.040, risk=0.347, classification error=0.000\n",
            ">epoch=29, lrate=0.040, risk=0.336, classification error=0.000\n",
            ">scenario=2, sigma=0.10, n=50, log_loss_mean=0.268, log_loss_std_dev=0.081, log_loss_min=0.000,         excess_risk=0.268, cls_error_mean=0.100, cls_error_std_dev=0.032\n",
            "[-0.908893   -0.14020584  0.43891074  0.04728973 -0.53936474]\n",
            ">epoch=0, lrate=0.040, risk=0.859, classification error=0.525\n",
            ">epoch=1, lrate=0.040, risk=0.838, classification error=0.517\n",
            ">epoch=2, lrate=0.040, risk=0.817, classification error=0.508\n",
            ">epoch=3, lrate=0.040, risk=0.796, classification error=0.505\n",
            ">epoch=4, lrate=0.040, risk=0.776, classification error=0.498\n",
            ">epoch=5, lrate=0.040, risk=0.756, classification error=0.495\n",
            ">epoch=6, lrate=0.040, risk=0.736, classification error=0.495\n",
            ">epoch=7, lrate=0.040, risk=0.717, classification error=0.493\n",
            ">epoch=8, lrate=0.040, risk=0.698, classification error=0.490\n",
            ">epoch=9, lrate=0.040, risk=0.679, classification error=0.485\n",
            ">epoch=10, lrate=0.040, risk=0.661, classification error=0.475\n",
            ">epoch=11, lrate=0.040, risk=0.643, classification error=0.458\n",
            ">epoch=12, lrate=0.040, risk=0.626, classification error=0.438\n",
            ">epoch=13, lrate=0.040, risk=0.608, classification error=0.405\n",
            ">epoch=14, lrate=0.040, risk=0.592, classification error=0.385\n",
            ">epoch=15, lrate=0.040, risk=0.575, classification error=0.353\n",
            ">epoch=16, lrate=0.040, risk=0.559, classification error=0.295\n",
            ">epoch=17, lrate=0.040, risk=0.543, classification error=0.250\n",
            ">epoch=18, lrate=0.040, risk=0.527, classification error=0.208\n",
            ">epoch=19, lrate=0.040, risk=0.512, classification error=0.168\n",
            ">epoch=20, lrate=0.040, risk=0.497, classification error=0.125\n",
            ">epoch=21, lrate=0.040, risk=0.483, classification error=0.080\n",
            ">epoch=22, lrate=0.040, risk=0.468, classification error=0.063\n",
            ">epoch=23, lrate=0.040, risk=0.455, classification error=0.043\n",
            ">epoch=24, lrate=0.040, risk=0.441, classification error=0.022\n",
            ">epoch=25, lrate=0.040, risk=0.428, classification error=0.018\n",
            ">epoch=26, lrate=0.040, risk=0.415, classification error=0.013\n",
            ">epoch=27, lrate=0.040, risk=0.402, classification error=0.007\n",
            ">epoch=28, lrate=0.040, risk=0.390, classification error=0.003\n",
            ">epoch=29, lrate=0.040, risk=0.378, classification error=0.003\n",
            ">scenario=2, sigma=0.10, n=100, log_loss_mean=0.298, log_loss_std_dev=0.099, log_loss_min=0.000,         excess_risk=0.298, cls_error_mean=0.147, cls_error_std_dev=0.042\n",
            "[ 0.65125388  0.20131912  0.67103324 -0.61283806  0.22847094]\n",
            ">epoch=0, lrate=0.040, risk=0.500, classification error=0.065\n",
            ">epoch=1, lrate=0.040, risk=0.485, classification error=0.043\n",
            ">epoch=2, lrate=0.040, risk=0.471, classification error=0.030\n",
            ">epoch=3, lrate=0.040, risk=0.457, classification error=0.022\n",
            ">epoch=4, lrate=0.040, risk=0.443, classification error=0.022\n",
            ">epoch=5, lrate=0.040, risk=0.430, classification error=0.013\n",
            ">epoch=6, lrate=0.040, risk=0.417, classification error=0.010\n",
            ">epoch=7, lrate=0.040, risk=0.404, classification error=0.005\n",
            ">epoch=8, lrate=0.040, risk=0.392, classification error=0.003\n",
            ">epoch=9, lrate=0.040, risk=0.380, classification error=0.000\n",
            ">epoch=10, lrate=0.040, risk=0.368, classification error=0.000\n",
            ">epoch=11, lrate=0.040, risk=0.357, classification error=0.000\n",
            ">epoch=12, lrate=0.040, risk=0.346, classification error=0.000\n",
            ">epoch=13, lrate=0.040, risk=0.335, classification error=0.000\n",
            ">epoch=14, lrate=0.040, risk=0.324, classification error=0.000\n",
            ">epoch=15, lrate=0.040, risk=0.314, classification error=0.000\n",
            ">epoch=16, lrate=0.040, risk=0.304, classification error=0.000\n",
            ">epoch=17, lrate=0.040, risk=0.294, classification error=0.000\n",
            ">epoch=18, lrate=0.040, risk=0.285, classification error=0.000\n",
            ">epoch=19, lrate=0.040, risk=0.275, classification error=0.000\n",
            ">epoch=20, lrate=0.040, risk=0.266, classification error=0.000\n",
            ">epoch=21, lrate=0.040, risk=0.258, classification error=0.000\n",
            ">epoch=22, lrate=0.040, risk=0.249, classification error=0.000\n",
            ">epoch=23, lrate=0.040, risk=0.241, classification error=0.000\n",
            ">epoch=24, lrate=0.040, risk=0.233, classification error=0.000\n",
            ">epoch=25, lrate=0.040, risk=0.226, classification error=0.000\n",
            ">epoch=26, lrate=0.040, risk=0.218, classification error=0.000\n",
            ">epoch=27, lrate=0.040, risk=0.211, classification error=0.000\n",
            ">epoch=28, lrate=0.040, risk=0.204, classification error=0.000\n",
            ">epoch=29, lrate=0.040, risk=0.197, classification error=0.000\n",
            ">scenario=2, sigma=0.10, n=500, log_loss_mean=0.165, log_loss_std_dev=0.031, log_loss_min=0.000,         excess_risk=0.165, cls_error_mean=0.004, cls_error_std_dev=0.000\n",
            "[-0.15683408 -0.39739329 -0.29845048 -0.02704723 -0.02100611]\n",
            ">epoch=0, lrate=0.040, risk=0.901, classification error=1.000\n",
            ">epoch=1, lrate=0.040, risk=0.879, classification error=1.000\n",
            ">epoch=2, lrate=0.040, risk=0.857, classification error=1.000\n",
            ">epoch=3, lrate=0.040, risk=0.836, classification error=1.000\n",
            ">epoch=4, lrate=0.040, risk=0.814, classification error=1.000\n",
            ">epoch=5, lrate=0.040, risk=0.794, classification error=1.000\n",
            ">epoch=6, lrate=0.040, risk=0.773, classification error=0.995\n",
            ">epoch=7, lrate=0.040, risk=0.753, classification error=0.975\n",
            ">epoch=8, lrate=0.040, risk=0.733, classification error=0.920\n",
            ">epoch=9, lrate=0.040, risk=0.714, classification error=0.740\n",
            ">epoch=10, lrate=0.040, risk=0.695, classification error=0.488\n",
            ">epoch=11, lrate=0.040, risk=0.676, classification error=0.265\n",
            ">epoch=12, lrate=0.040, risk=0.658, classification error=0.105\n",
            ">epoch=13, lrate=0.040, risk=0.640, classification error=0.043\n",
            ">epoch=14, lrate=0.040, risk=0.623, classification error=0.020\n",
            ">epoch=15, lrate=0.040, risk=0.605, classification error=0.007\n",
            ">epoch=16, lrate=0.040, risk=0.588, classification error=0.000\n",
            ">epoch=17, lrate=0.040, risk=0.572, classification error=0.000\n",
            ">epoch=18, lrate=0.040, risk=0.556, classification error=0.000\n",
            ">epoch=19, lrate=0.040, risk=0.540, classification error=0.000\n",
            ">epoch=20, lrate=0.040, risk=0.524, classification error=0.000\n",
            ">epoch=21, lrate=0.040, risk=0.509, classification error=0.000\n",
            ">epoch=22, lrate=0.040, risk=0.494, classification error=0.000\n",
            ">epoch=23, lrate=0.040, risk=0.480, classification error=0.000\n",
            ">epoch=24, lrate=0.040, risk=0.466, classification error=0.000\n",
            ">epoch=25, lrate=0.040, risk=0.452, classification error=0.000\n",
            ">epoch=26, lrate=0.040, risk=0.438, classification error=0.000\n",
            ">epoch=27, lrate=0.040, risk=0.425, classification error=0.000\n",
            ">epoch=28, lrate=0.040, risk=0.412, classification error=0.000\n",
            ">epoch=29, lrate=0.040, risk=0.400, classification error=0.000\n",
            ">scenario=2, sigma=0.10, n=1000, log_loss_mean=0.314, log_loss_std_dev=0.110, log_loss_min=0.000,         excess_risk=0.314, cls_error_mean=0.176, cls_error_std_dev=0.130\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}